<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PerAct: A Multi-Task Transformer for Robotic Manipulation">
  <meta name="keywords" content="Transformers, Language Grounding, Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PerAct</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Perceiver-Actor: A Multi-Task Transformer <br> for Robotic Manipulation</h1>
          <h3 class="title is-4 conference-authors"><a target="_blank" href="">Anonymous Submission</a></h3>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="paper/peract_corl2022.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
            </div>

          </div>
<!--           <br>
          <br> -->


        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop height="100%">
            <source src="media/intro/sim_rolling_v2.mp4"
                    type="video/mp4">
          </video>
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        <span class="dperact">PerAct</span> is an end-to-end behavior-cloning agent that can learn <br>a single language-conditioned policy for 18 RLBench tasks (with 249 unique task variations)
        </h2>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop height="100%">
            <source src="media/intro/1_handsan.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop height="100%">
            <source src="media/intro/2_food_bin.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="media/intro/4_drawer.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
            <source src="media/intro/3_marker.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="media/intro/5_stick.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="media/intro/7_sweeping.mp4"
                    type="video/mp4">
          </video>
        </div>
       <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop height="100%">
            <source src="media/intro/6_blocks.mp4"
                    type="video/mp4">
          </video>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>
<h2 class="subtitle has-text-centered">
</br>
  We also train <b>one multi-task Transformer</b> <i>from scratch</i> on 7 real-world tasks with just <b>53 demos</b> in total.
</h2>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Transformers have revolutionized vision and natural language processing 
            with their ability to scale with large datasets. But in robotic manipulation, 
            data is both limited and expensive. Can we still benefit from Transformers with the right problem formulation? 
          </p>
          <p>
            We investigate this question with <span class="dperact">PerAct</span>, a
            language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation.
            <span class="dperact">PerAct</span> encodes language goals and RGB-D voxel observations with a 
            <a target=”_blank” href="https://www.deepmind.com/blog/building-architectures-that-can-handle-the-worlds-data">Perceiver Transformer</a>, and outputs discretized 
            actions by “detecting the next best voxel action”. Unlike frameworks that operate on 2D images, the voxelized observation
            and action space provides a strong structural prior for efficiently learning 6-DoF
            policies. 
          </p>
          <p>
            With this formulation, we train <b>a single multi-task Transformer</b> for 18
            RLBench and 7 real-world tasks with just a few demonstrations per task. Our
            results show that We investigate this question with <span class="dperact">PerAct</span> significantly outperforms unstructured image-to-action
            agents and 3D ConvNet baselines for a wide range of tabletop tasks.
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>
    <!--/ Abstract. -->

  </div>

  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <iframe src="https://www.youtube.com/embed/-u3RVaUJjgk?rel=0&amp;showinfo=0"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="dperact">PerAct</span></h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Action-Centric Transformer</h3>
        <div class="content has-text-justified">
        <br>
        </div>
        <img src="media/figures/arch.png" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </br>
        </br>
          <p>
             <span class="dperact">PerAct</span> takes as input a language goal and a voxel grid reconstructed from RGB-D sensors. The voxels are split into 3D patches, and the language goal is encoded with a pre-trained language model. These language and voxel features are appended together as a sequence and encoded with a <a target=”_blank” href="https://www.deepmind.com/blog/building-architectures-that-can-handle-the-worlds-data">Perceiver Transformer</a>. Despite the extremely long input sequence, Perceiver uses a small set of latent vectors to encode the input. These encodings are upsampled back to the original voxel dimensions with a decoder and reshaped with linear layers to predict discretized translation, rotation, gripper open, and collision avoidance actions. This action is executed with a motion-planner after which the new observation is used to predict the next discrete action in an observe-act loop until termination.
          </p>

        <br/>
        <br/>

      </div>
    </div>

  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
